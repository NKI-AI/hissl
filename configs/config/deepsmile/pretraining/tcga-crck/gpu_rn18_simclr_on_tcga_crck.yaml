# @package _global_
config:
  VERBOSE: False
  LOG_FREQUENCY: 1
  TEST_ONLY: False
  TEST_MODEL: False
  SEED_VALUE: 42
  MULTI_PROCESSING_METHOD: forkserver
  MONITOR_PERF_STATS: True
  PERF_STAT_FREQUENCY: 10
  ROLLING_BTIME_FREQ: 5
  DATA:
    NUM_DATALOADER_WORKERS: 0
    TRAIN:
      DATA_SOURCES: [disk_filelist]
      DATA_PATHS: [/hissl/tools/reproduce_deepsmile/2_download_tcga_crck/train_paths.npy]
      REMOVE_IMG_PATH_PREFIX: "train/"
      NEW_IMG_PATH_PREFIX: "/hissl/tools/reproduce_deepsmile/2_download_tcga_crck/train/"
      DATASET_NAMES: [imagenet1k_folder] # needed because of a funky assert in VISSL that shouldn't be there.
      PREFETCH_FACTOR: 2
      BATCHSIZE_PER_REPLICA: 2
      LABEL_TYPE: sample_index    # just an implementation detail. Label isn't used
      TRANSFORMS:
        - name: ImgReplicatePil
          num_times: 2
        - name: RandomResizedCrop
          size: 224
        - name: RandomHorizontalFlip
          p: 0.5
        - name: ImgPilColorDistortion # has both color jitter and grayscale
          strength: 1.0
        - name: ToTensor
      COLLATE_FUNCTION: simclr_collator
      MMAP_MODE: True
      COPY_TO_LOCAL_DISK: False
      DATA_LIMIT: -1
      DROP_LAST: True
  TRAINER:
    TRAIN_STEP_NAME: standard_train_step
  METERS:
    name: ""
  MODEL:
    TRUNK:
      NAME: resnet
      RESNETS:
        DEPTH: 18
    HEAD:
      PARAMS: [
        ["mlp", {"dims": [512, 512], "use_relu": True}],
        ["mlp", {"dims": [512, 128]}],
      ]
    SYNC_BN_CONFIG:
      CONVERT_BN_TO_SYNC_BN: True
      SYNC_BN_TYPE: pytorch
    AMP_PARAMS:
      USE_AMP: False
      AMP_ARGS: {"opt_level": "O3", "keep_batchnorm_fp32": True, "master_weights": True, "loss_scale": "dynamic"}
      AMP_TYPE: pytorch
    FSDP_CONFIG: # TAKEN FROM DEFAULTS.YAML
        # set this option to True to enable FSDP and automatically determine the config
        # for FSDP based on AMP true/false.
        AUTO_SETUP_FSDP: False
        # Set this option to a positive number to automatically wrap "big" layers with
        # a dedicated FSDP wrapping: the number provided here is the number of
        # parameters that serves as threshold to decide if a layer is "big"
        AUTO_WRAP_THRESHOLD: 0
        # Parameters of fairscale FSDP
        flatten_parameters: True
        mixed_precision: True
        fp32_reduce_scatter: False  # Only makes sense to be True when mixed_precision is True.
        compute_dtype: float32  # Choose "float32" or "float16"
        bucket_cap_mb: 0
        clear_autocast_cache: True
        verbose: True
        AMP_TYPE: O1 # Not used, but hydra_config wants to delete this and otherwise it throws an error
  LOSS:
    name: simclr_info_nce_loss
    simclr_info_nce_loss:
      temperature: 0.5
      buffer_params:
        embedding_dim: 128
  OPTIMIZER:
    name: "adam"
    # whether to shard optimizer state as per ZeRO https://arxiv.org/abs/1910.02054
    use_zero: False
    use_larc: False  # supported for SGD only for now
    larc_config:
      clip: False
      eps: 1e-08
      trust_coefficient: 0.001
    weight_decay: 0.0001
    momentum: 0.9
    nesterov: False
    # for how many epochs to do training. only counts training epochs.
    num_epochs: 100
    betas: [.9, .999] # for Adam/AdamW
    # whether to regularize batch norm. if set to False, weight decay of batch norm params is 0.
    regularize_bn: False
    # whether to regularize bias parameter. if set to False, weight decay of bias params is 0.
    regularize_bias: True
    # Parameters to omit from regularization. Any named parameter whose name
    # contains any of these strings will be omitted from regularization.
    # For example, we don't want to regularize the class token or position
    # embeddings in the vision transformer, so we pass:
    # non_regularized_parameters: ['class_token', 'pos_embedding']
    non_regularized_parameters: []
    # we support using a different LR and weight decay for head and trunk.
    # one needs to set the flag "use_different_values: True" in order to enable
    # this functionality. We use the same type of param scheduler for the trunk and head
    # but allow different LR and weight decay values.
    head_optimizer_params:
      # if the head should use a different LR than the trunk. If yes, then specify the
      # param_schedulers.lr_head settings. Otherwise if set to False, the
      # param_scheduelrs.lr will be used automatically.
      use_different_lr: False
      # if the head should use a different weight decay value than the trunk.
      use_different_wd: False
      # if using different weight decay value for the head, set here. otherwise, the
      # same value as trunk will be automatically used.
      weight_decay: 0.0001
    param_schedulers:
      lr:
        # we make it convenient to scale Learning rate automatically as per the scaling
        # rule specified in https://arxiv.org/abs/1706.02677 (ImageNet in 1Hour).
        auto_lr_scaling:
          # if set to True, learning rate will be scaled.
          auto_scale: False
          # base learning rate value that will be scaled.
          base_value: 0.0001
          # batch size for which the base learning rate is specified. The current batch size
          # is used to determine how to scale the base learning rate value.
          # scaled_lr = ((batchsize_per_gpu * world_size) * base_value ) / base_lr_batch_size
          base_lr_batch_size: 256
          # scaling_type can be set to "sqrt" to reduce the impact of scaling on the base value
          scaling_type: "linear"
        name: "constant"
        update_interval: "epoch"
        # values indicate the step LR learning rate values. Instead of taking gamma, we
        # take the actual LR value to use. This allows freedom to not having to use
        # a fixed gamma.
        values: []
        # milestones denotes the epochs at which learning rate is decayed.
        milestones: []
        # The below parameters are valid for lr.name = "composite". Various schedulers
        # can then be composed together for the training. For example: linear warmup +
        # multistep schedule after warmup.
        schedulers: []
        interval_scaling: []
        lengths: []
        # =====cosine learning rate specific =======
        start_value: 0.0001
        end_value: 0.0001
        # =====constant learning rate specific =======
        value: 0.0001
      # if we want to use a different LR scheduler for the head, then specify
      # the lr_head similar to "lr"
      lr_head:
        auto_lr_scaling:
          auto_scale: False
          base_value: 0.1
          base_lr_batch_size: 256
          scaling_type: "linear"
        name: "multistep"
        update_interval: "epoch"
        values: [0.1, 0.01, 0.001]
        milestones: [30, 60]
        # The below parameters are valid for lr.name = "composite". Various schedulers
        # can then be composed together for the training. For example: linear warmup +
        # multistep schedule after warmup.
        schedulers: []
        interval_scaling: []
        lengths: []
        # =====cosine learning rate specific =======
        start_value: 0.1
        end_value: 0.0
        # =====constant learning rate specific =======
        value: 0.1
    # Under certain cases, we want to use a single param_group that consists of
    # the parameters returned from model.parameters() list. For example, when a model
    # is wrapped by FSDP with flattening, individual parameters inside the model are
    # replaced with a flattened parameter. Therefore, this option is needed in that case.
    construct_single_param_group_only: False
  DISTRIBUTED:
    BACKEND: nccl
    NUM_NODES: 1
    NUM_PROC_PER_NODE: 1
    INIT_METHOD: tcp
    RUN_ID: auto
  MACHINE:
    DEVICE: gpu
  CHECKPOINT:
    DIR: ""
    AUTO_RESUME: True
    CHECKPOINT_FREQUENCY: 5
    OVERWRITE_EXISTING: false
  HOOKS:
    # ----------------------------------------------------------------------------------- #
    # Perf hooks for several steps of model training
    # ----------------------------------------------------------------------------------- #
    PERF_STATS:
      # monitoring training statistics like: forward time, backward time, loss time, etc
      MONITOR_PERF_STATS: True
      # we print perf stats (if enabled) after every phase. If we want to print every few
      # batches, set the frequency here.
      PERF_STAT_FREQUENCY: -1
      # if we want to print the rolling average batch time, set the value below to number of
      # training iterations over which we want to print average. The average is printed for
      # master gpu.
      ROLLING_BTIME_FREQ: -1
  # ----------------------------------------------------------------------------------- #
  # PROFILING
  # ----------------------------------------------------------------------------------- #
  PROFILING:
    # Defaults params are used from defaults.yaml
    RUNTIME_PROFILING:
      # To enable the runtime profiler
      USE_PROFILER: False

